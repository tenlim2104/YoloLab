{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwc-xQo_80Ko"
      },
      "source": [
        "# Object Detection with YOLO\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eagleshot/CustomYOLOModel/blob/main/yolo.ipynb)\n",
        "\n",
        "Hello! In this tutorial, we will look at object detection with YOLO (You Only Look Once). YOLO is a state-of-the-art, real-time object detection algorithm, known for its speed and accuracy. It combines object classification and localization into a single neural network, making it highly efficient.\n",
        "\n",
        "In this tutorial, we will cover the following topics:\n",
        "\n",
        "* Introduction to object detection with YOLO.\n",
        "* Using pre-trained models for object detection.\n",
        "* Training a custom model on your own dataset.\n",
        "* Outlook: Deploy your model on different hardware.\n",
        "\n",
        "By the end of this tutorial, you will have an understanding of how to use YOLO and will be able to apply it to various object detection tasks.<br>\n",
        "While we won't dive into the algorithm's technical details, you can learn more from the\n",
        "[original paper](https://arxiv.org/pdf/1506.02640v5) if you are interested. The YOLO architecture was originally released in 2016 and has since been constantly improved and adapted, to work on different tasks, such as object detection, image segmentation, and pose estimation, making it very useful for different tasks.So let's dive in and get started! <br><br>\n",
        "\n",
        "![banner](https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png)\n",
        "\n",
        "## Installation\n",
        "First, we need to install and import the required packages. Using the `%` sign in front of the command allows you to run shell commands in the jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyA1aRly80Kr"
      },
      "outputs": [],
      "source": [
        "# Install Ultralytics YOLO\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "ultralytics.checks()\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cquRbMQ80Ks"
      },
      "source": [
        "Ultralytics is a company, that provides a easy to use python package for YOLO, that allows you to use pre-trained models, train your own models as well as deploy them. It is licensed under an open source [AGPL-3.0 license](https://www.ultralytics.com/license) which is perfect for research and educational purposes. However, if you need to use YOLO for a commercial project, you may want to consider using another implementation (e.g. [YoloV9 MIT](https://github.com/WongKinYiu/YOLO)) or purchase a license from Ultralytics.\n",
        "\n",
        "As we need a graphics card to run YOLO at a reasonable speed, please make sure that the GPU is detected. Otherwise, you may need to change the runtime type in Google Colab. Make sure the output above is similar to the one below.\n",
        "\n",
        "```python\n",
        "Ultralytics YOLOv8.2.91 üöÄ Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
        "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 32.6/112.6 GB disk)\n",
        "```\n",
        "\n",
        "Before we start with detecting objects, let's quickly look at how YOLO works in python, as it always follows the same pattern:\n",
        "\n",
        "\n",
        "### 1. Load the model\n",
        "First of all, you need to load a model to use it for object detection. You can decide which model you want to use, depending on the task you want to perform. You can find a list of available pre-trained models [here](https://docs.ultralytics.com/models/) and [here](https://docs.ultralytics.com/tasks/). These models have been trained on a large dataset of 80 different objects and are accurate enough for many use cases. However, as we will look at later, you can also train or fine-tune your own model and use it the same way.\n",
        "\n",
        "![yolo-models](https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/yolo-comparison-plots.png)\n",
        "\n",
        "\n",
        "As you can see in the graph above, the YOLO archtecture is constantly evolving and there are different model generations (e.g. YOLOV8, YOLOV10), that try to improve the detection accuracy and efficiency. The Y-Axis shows the mAP (mean average precision) of the model using a standardized dataset (COCO). This is a measure of the detection accuracy. The X-Axis shows the model size and inference time, which is a measure of the speed of the model. There are different sizes of each model (e.g. nano, small, medium, balanced, large, xlarge). The larger the model, the more accurate it is, while also being slower and requiring more memory. Furthermore, there also is a diminishing return with larger models, so they cannot be infinitely scaled. Depending on your use case and hardware, you need to choose the right model and find the balance between speed and accuracy.\n",
        "\n",
        "```python\n",
        "model = YOLO('yolov8n.pt')\n",
        "```\n",
        "\n",
        "To import a pre-trained model, you can use the code above and replace the model name with the one you want to use. The model will be downloaded automatically, if it is not already available in the cache. You can also specify a path to a local model (e.g. when using your own model).\n",
        "\n",
        "\n",
        "### 2. Inference\n",
        "Inference is the process of using the model to make predictions on new data. You can do this with a wide variety of [input sources](https://docs.ultralytics.com/modes/predict/#inference-sources) like images, videos, live streams, that are supported out of the box. Most popular [image and video formats](https://docs.ultralytics.com/modes/predict/#image-and-video-formats) are supported. Using [inference arguments](https://docs.ultralytics.com/modes/predict/#inference-arguments), you can customize the inference process (e.g. which classes to detect, confidence threshold, etc.) and the visualization. The `predict()` call returns a list of Results objects.\n",
        "\n",
        "```python\n",
        "# You can predict on a single image by passing the path or URL to the image\n",
        "results = model('image.jpg')\n",
        "\n",
        "# You can also use batch processing by passing a list of images to the model (faster)\n",
        "results = model([\"im1.jpg\", \"im2.jpg\"])\n",
        "\n",
        "# You can do the same with videos or live streams\n",
        "results = model('video.mp4')\n",
        "```\n",
        "\n",
        "### 3. Process the results\n",
        "The ```predict()``` call returns a list of Results objects, that contain the detected objects. You can access the detected objects by iterating over the results and accessing the bounding boxes and other information. This is heavily dependent on the task you are trying to solve, so we will look at it later with several exercises. Below is a simple function to plot the detected objects on an image that you can use to visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4E4KiPx80Ks"
      },
      "outputs": [],
      "source": [
        "def plot_results(results):\n",
        "    img = results[0].plot()\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert image to RGB\n",
        "    plt.imshow(img_rgb)  # Display results\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwMm86en80Kt"
      },
      "source": [
        "Technically, you can also use the command line interface (CLI) to run YOLO with the same functionality. This is useful, if you want to run YOLO from the terminal and don't have a python environment available. You can find more information about the command line interface [here](https://docs.ultralytics.com/usage/cli/). Below is an example, that also does the same as the python code above:\n",
        "\n",
        "```bash\n",
        "yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n",
        "```\n",
        "<br>\n",
        "\n",
        "## Image classification\n",
        "![classification](https://user-images.githubusercontent.com/26833433/243418606-adf35c62-2e11-405d-84c6-b84e7d013804.png)\n",
        "With [image classification](https://docs.ultralytics.com/tasks/classify/), you can classify an entire image into a set of predefined classes. The image classifier outputs a class label for an image and a confidence score for that class. This is useful, if you efficiently want to classify an image and don't need to know where in the image the object is located. Classification models have the suffix ```-cls``` in their name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9vGyGY80Kt"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8m-cls.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
        "\n",
        "# Display results (doesn't do a lot here)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS3D5aDP80Kt"
      },
      "source": [
        "*Please note that the pre-trained classification models were trained on a different dataset than the detection models, so they may not perform the same.*<br>\n",
        "\n",
        "## Object detection\n",
        "![detection](https://user-images.githubusercontent.com/26833433/243418624-5785cb93-74c9-4541-9179-d5c6782d491a.png)\n",
        "With [object detection](https://docs.ultralytics.com/tasks/detect/), you can identify the class and location of objects in an image or video stream. The object detector outputs bounding boxes and class labels for each detected object as well as a confidence score for each detection. The confidence score indicates how confident the model is that the object is correctly detected. This is useful if you need to know where the object is located in the image.\n",
        "\n",
        "**Exercise 1:** Modify the code below. Test different model sizes and generations on different images. Do you notice a difference in detection speed and accuracy?\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "\n",
        "You can test different models by changing the model name in the code below. You can also test different images by changing the image URL or upload an image to the storage. You can find a list of available models [here](https://docs.ultralytics.com/models/). Example:\n",
        "```python\n",
        "model = YOLO('yolov8n.pt')\n",
        "model = YOLO('yolov8x.pt')\n",
        "model = YOLO('yolov10n.pt')\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 2:**  Try to only detect humans with a confidence score of at least 70%.<br>\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "You can find the classes online or with the `model.names` attribute:\n",
        "\n",
        "```python\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://ultralytics.com/images/zidane.jpg\", classes=0, conf=0.7)\n",
        "model.names\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 3:** Output all the detected objects with their class name and confidence score. *Hint: They are stored in the bounding boxes in the `results[0].boxes` attribute.*\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "for box in results[0].boxes:\n",
        "    cls = box.cls # Class index\n",
        "    class_label = model.names[int(cls)] # get the class label\n",
        "    conf = box.conf.item() # Confidence\n",
        "    print(f\"Class: {class_label} - Confidence: {conf:.2f}\")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbbP7gbm80Ku"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained model\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://ultralytics.com/images/zidane.jpg\")\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1c9qFoz80Ku"
      },
      "source": [
        "**Parking spot exercise üÖøÔ∏è:** Let's try to detect cars in an image and check if they are parked in a specific parking spot. The image below shows a parking spot with multiple cars parked.\n",
        "\n",
        "**Exercise 1:** Print the number of detected cars in the image below.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://upload.wikimedia.org/wikipedia/commons/7/72/Bombala_-_backward_parking_cars.jpg\", classes=2)\n",
        "print(f\"Detected {len(results[0])} cars.\")\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 2:** Get the bounding box coordinates and the width and height of the bounding boxes of the detected cars in the image below.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "for box in results[0].boxes:\n",
        "    w = box.xywh[0][2].item() # width\n",
        "    h = box.xywh[0][3].item() # height\n",
        "    x = box.xywh[0][0].item() - w/2 # x-coordinate\n",
        "    y = box.xywh[0][1].item() - h/2 # y-coordinate\n",
        "    print(f\"Box at ({x}, {y}) with width {w} and height {h}\")\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 3:** Make a program that checks, if a car is on the parking spot below the sign on the right using the coordinates of the bounding box.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://upload.wikimedia.org/wikipedia/commons/7/72/Bombala_-_backward_parking_cars.jpg\", classes=2)\n",
        "\n",
        "def plot_box(x, y, w, h, img, title=\"\", color='r'):\n",
        "    # Define Matplotlib figure and axis\n",
        "    _, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
        "    ax.set_title(title, color=color)\n",
        "    ax.imshow(img_rgb)\n",
        "    \n",
        "    # Add rectangle to plot\n",
        "    rect = Rectangle((x, y), w, h, linewidth=1, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Display plot\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "parking_spot = [1350, 600, 200, 175]\n",
        "\n",
        "plot_box(*parking_spot, results[0].orig_img, title=\"Parking spot\", color='b')\n",
        "\n",
        "for box in results[0].boxes:\n",
        "    w = box.xywh[0][2].item() # width\n",
        "    h = box.xywh[0][3].item() # height\n",
        "    x = box.xywh[0][0].item() - w/2 # x-coordinate\n",
        "    y = box.xywh[0][1].item() - h/2 # y-coordinate\n",
        "\n",
        "    img = results[0].orig_img # Get the original image\n",
        "\n",
        "    # Check if the two boxes intersect\n",
        "    if (x < parking_spot[0] + parking_spot[2] and x + w > parking_spot[0] and y < parking_spot[1] + parking_spot[3] and y + h > parking_spot[1]):\n",
        "        plot_box(x, y, w, h, img, title=\"Car is inside the parking spot.\", color='g')\n",
        "    else:\n",
        "        plot_box(x, y, w, h, img, title=\"Car is not inside the parking spot.\", color='r')\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO5UWYKn80Ku"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://upload.wikimedia.org/wikipedia/commons/7/72/Bombala_-_backward_parking_cars.jpg\")\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJxRVtE80Kv"
      },
      "source": [
        "## Instance segmentation\n",
        "![segmentation](https://user-images.githubusercontent.com/26833433/243418644-7df320b8-098d-47f1-85c5-26604d761286.png)\n",
        "With [instance segmentation](https://docs.ultralytics.com/tasks/segment/#predict), you can identify the class and location of an object in an image or video stream and segment it from the background. This is useful if you need to know the exact position and shape of the object. Segmentation models have the suffix `-seg` in their name.\n",
        "\n",
        "**Exercise 1:** Do instance segmentation on an image in the code cell below.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://ultralytics.com/images/zidane.jpg\")\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 2:** Get and plot the binary segmentation masks of the detected objects in the image below.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Get binary segmentation masks\n",
        "for mask in results[0].masks.xy:\n",
        "    binary_mask = np.zeros(results[0].orig_shape, dtype=np.uint8)\n",
        "\n",
        "    contour = mask.astype(np.int32)\n",
        "    contour = contour.reshape(-1, 1, 2)\n",
        "\n",
        "    binary_mask = cv2.drawContours(binary_mask, [contour], -1, (255, 255, 255), cv2.FILLED)\n",
        "\n",
        "    # Plot binary mask\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(binary_mask, cmap='gray')\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "```\n",
        "</details>\n",
        "\n",
        "**Exercise 3:** Compare the speed of classification, detection and segmentation using the \"same\" model architecture on the same image. Also try using the CPU instead of the GPU for inference (can be configured in the model call).\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "  \n",
        "```python\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "# Predict on an image\n",
        "results = model(\"https://ultralytics.com/images/zidane.jpg\", device='cpu')\n",
        "\n",
        "# Plot the results\n",
        "plot_results(results)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwe9eM3f80Kv"
      },
      "outputs": [],
      "source": [
        "# Insert your code here ‚úèÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqzo6cSu80Kv"
      },
      "source": [
        "## Object tracking\n",
        "![tracking](https://github.com/ultralytics/docs/releases/download/0/multi-object-tracking-examples.avif)\n",
        "With [object tracking](https://docs.ultralytics.com/modes/track/), you can track the location of an object in a video or video stream over time. The object tracker outputs bounding boxes for each frame as well as a unique ID for each object. You can process the output similar to the object detection results. This is useful if you need to know the trajectory of an object or want to count objects over time. Since the tracker is built on top of the detection model, you can use the same models for tracking as well as detection (bounding boxes, segmentation, pose estimation).\n",
        "\n",
        "**Exercise:** Track objects in a video in the code cell below. Save the video to a file (showing a video in Google Colab is difficult).\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "    \n",
        "```python\n",
        "# Load a pre-trained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "results = model.track(\"https://www.youtube.com/watch?v=CftLBPI1Ga4\", stream_buffer=True, save=True)  # Tracking with default tracker\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R5qVhrM80Kv"
      },
      "outputs": [],
      "source": [
        "# Insert your code here ‚úèÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRtl0yPD80Kv"
      },
      "source": [
        "## Pose estimation\n",
        "![pose estimation](https://github.com/ultralytics/docs/releases/download/0/pose-estimation-examples.avif)\n",
        "With [pose estimation](https://docs.ultralytics.com/tasks/pose/), you can identify the pose of a person in an image or video stream. The pose estimator outputs key points for each person detected as well as a confidence score for each key point. This is useful if you need to know the position of a person's body parts (e.g. eyes, shoulders, hips).\n",
        "\n",
        "**Exercise:** Estimate the pose of a person in an image in the code cell below.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "    \n",
        "```python\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n-pose.pt\")\n",
        "\n",
        "# Predict with the model\n",
        "results = model(\"https://img.freepik.com/free-photo/group-people-performing-stretching-exercise_1170-116.jpg\")\n",
        "\n",
        "# Display the results\n",
        "plot_results(results)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNOITQe280Kv"
      },
      "outputs": [],
      "source": [
        "# Insert your code here ‚úèÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaHPv3Ho80Kw"
      },
      "source": [
        "## Custom Object Detection üèîÔ∏è‚õ∑Ô∏è\n",
        "Now that you have mastered the basics of object detection with YOLO, it's time to train your own model! You have been tasked by a ski resort in the Grisons mountain to develop a system that can detect and count the number of skiers on the ski slopes in real-time. Because of this, we need to train our custom YOLO model.<br>\n",
        "\n",
        "### Transfer learning\n",
        "While you could train a model from scratch it is often much more efficient to use an existing model and fine-tune it on your own dataset. This process, where a model trained on one task is re-purposed on a second related task, is called transfer learning. Since the model has already learned to detect and classify objects in images, it can be relatively easily be adapteded to detect new objects, that it hasn't seen before. Transfer learning is widely used, because it uses much less training data, trains much faster and often achieves better performance than training a model from scratch.<br>\n",
        "\n",
        "### Dataset\n",
        "In order to train our custom YOLO model, we need a dataset of labeled images with the objects we want to classify. This is what we call supervised learning. There are many datasets with thousands of images available online, that are already labeled and can be used or expanded. Some of them are generic, while others are application specific (e.g. medical imaging, aerial imaging, self-driving cars etc.). Here are two examples of popular datasets:\n",
        "* https://cocodataset.org/#explore\n",
        "* https://storage.googleapis.com/openimages/web/index.html\n",
        "\n",
        "The COCO dataset for example, that was used to train the original YOLO model contains images of 80 popular objects. Take a minute to look at the website and see how the images are labelled. For this exercise however, we want to create our own dataset from scratch. Creating your own dataset can be a very time consuming process. Fortunately, there are a lot of images of skiers available online, that we can use for our model.\n",
        "\n",
        "### Synthetic data\n",
        "![Unreal GT](https://unrealgt.github.io/images/image_overview.png)\n",
        "\n",
        "Sometimes it can be difficult or tedious, to get enough labeled images for training your model. This can have several reasons:\n",
        "\n",
        "* Real data can be hard to collect (e.g. a defect in a production line that only occurs very rarely)\n",
        "* The amount of different objects to detect is very large\n",
        "* The amount of objects will grow over time and the model needs to be retrained with new data\n",
        "* ...\n",
        "\n",
        "Furthermore, labelling can also be difficult. In some cases, not even experts can agree on the correct label (e.g. for classifying a defect). In these cases, it can help to generate synthetic data. This is data that is artificially created by a computer program (e.g. by rendering a scene). Once setup, synthetic data can be generated in large quantities with different lighting conditions, backgrounds, positioning, etc. which is perfect for training our models. Since the ground truth is known, the labels can be generated automatically. Therefore it can be very useful for training a model and - depending of the application - save you a lot of time. How you generate the data is highly dependent on the task you are trying to solve. You can several example of the usage of synthetic data [here](https://blogs.nvidia.com/blog/what-is-synthetic-data/). Depending on the task, you can also use relatively simple methods to create synthetic data, like in [this study on tomato detection](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5426829/).\n",
        "\n",
        "As always with training object detection models, it's good to take an iterative approach and test the model with real world data early on and then improve the model with more data and fine-tuning.\n",
        "\n",
        "## Training your custom model\n",
        "For this exercise, we will be doing this the traditional way and labelling the images ourselves. I have prepared a small dataset of images of skiers for you to use. Execute the code below to download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gqznIPe80Kw"
      },
      "outputs": [],
      "source": [
        "# Download training data\n",
        "!wget https://github.com/Eagleshot/CustomYOLOModel/archive/refs/heads/main.zip --quiet\n",
        "!unzip -q main.zip\n",
        "!mv /content/CustomYOLOModel-main/ /content/datasets/ # Rename folder\n",
        "PATH = \"/content/datasets/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64RUajHl80Kw"
      },
      "source": [
        "Take a look at the dataset we downloaded. The data.yaml file describes the dataset and its location on the hard drive. Since we only have one class, we only need to specify the name of the class (skier).\n",
        "\n",
        "```yaml\n",
        "path: ./SkiDataset\n",
        "train: ./train\n",
        "val: ./val\n",
        "test:\n",
        "\n",
        "names:\n",
        "  0: skier\n",
        "```\n",
        "\n",
        "The dataset is split into a training (70%) and a validation set (30%). The training set is used to train the model and adjust the weights of the model based on the images and corresponding labels. The validation set is used periodically during training to monitor the models performance on unseen data. This is important to prevent overfitting, where the model \"memorizes\" the training data and doesn't work well anymore on new data (generalization). The validation data itself is not used to train the model. The test data is only used, after the model has finished training, to evaluate the model on completely new data. As we only have a very small dataset, we will not use a test set in this exercise as it does not improve the model performance. We also have several background files (10%) that don't contain any skiers and therefore don't have any labels. They are not necessarily needed, but can help to improve the model performance and reduce false positives. As you may have seen, there are also several images of regular people in the dataset. Do you know why they are in the dataset? Keep them in the back of your mind for later.\n",
        "\n",
        "Now we look at the images and labels in the dataset. For each image, there is a text file with the labels for the bounding box of the image. The labels are stored in the following format: `class x_center y_center width height`.\n",
        "\n",
        "```txt\n",
        "0 0.458187 0.537281 0.405458 0.425439\n",
        "0 0.692593 0.161550 0.076998 0.130117\n",
        "0 0.610721 0.155702 0.067251 0.165205\n",
        "```\n",
        "The coordinates are normalized to the range [0, 1] and are therefore independent of the image size. Each line in the label file corresponds to one object in the image. As we only have one class, all objects begin with 0.\n",
        "\n",
        "![label](https://github.com/ultralytics/docs/releases/download/0/two-persons-tie.avif)\n",
        "\n",
        "To label images yourself, you can use a tool to import the images, draw the bounding boxes and export the labels in the correct format. There are many different tools available - I recommend using [makesense.ai](https://www.makesense.ai/), which is free and open-source. There you can import the images, create the label(s) (in this case only one - skier), draw the bounding boxes and then export the labels (annotations) in the YOLO format using the actions button. Then you can simply import the labels into the correct folder in the dataset.\n",
        "\n",
        "When creating or searching for images for your own dataset, there are a few things you need to consider: In order to train a robust model, you need a diverse dataset. This means that you want images from different angles, lighting conditions, distances, etc. This will make your model more robust and generalizable. If you know that your model will be used in a specific environment, you should also try to collect images from that environment.\n",
        "\n",
        "**Exercise 1:** Now its your turn - get some more images of skiers and label them. Put them into the dataset folder and do a train/test split. Label at least 25 images. If you want, you can share the labeled images with me.\n",
        "\n",
        "**Exercise 2:** Train the model using the code below. This will some time to run. While the model is training, look at the training folder in `/runs/detect/train`. A few files (labels.jpg, labels_correlogram.jpg, train_batch0.jpg etc.) have automatically been created there. Can you figure out what they tain_batch images are showing?\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "\n",
        "The train_batch images show the augmented images that are used to train the model. Data augmentation is a technique used to artificially increase the size of the training dataset by applying many different transformations to the images like changing its hue, saturation and value (brightness) of the image as well as rotating, scaling, flipping them and many more. This helps to improve the performance of the model, as the training data is used better. You can also use it to check if the labels are recognized correctly.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An3gdw_E80Kw"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained model for transfer learning\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Train the model for 100 epochs\n",
        "results = model.train(data = PATH + \"data.yaml\", epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRxpYI2D80Kw"
      },
      "source": [
        "**Exercise 3:** After the model has finished training, look at the files that were generated. There are a few metrics, that are used to evaluate how successfull the training was. Look up what they mean. You will look at them more in-depth later this semester.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "\n",
        "* Intersection over Union (IoU): IoU is a measure that quantifies the overlap between a predicted bounding box and a ground truth bounding box. It plays a fundamental role in evaluating the accuracy of object localization.\n",
        "\n",
        "* Precision and Recall (P and R): Precision quantifies the proportion of true positives among all positive predictions, assessing the model's capability to avoid false positives. On the other hand, Recall calculates the proportion of true positives among all actual positives, measuring the model's ability to detect all instances of a class.\n",
        "\n",
        "* Average Precision (AP): AP computes the area under the precision-recall curve, providing a single value that encapsulates the model's precision and recall performance.\n",
        "\n",
        "* Mean Average Precision (mAP): mAP extends the concept of AP by calculating the average AP values across multiple object classes. This is useful in multi-class object detection scenarios to provide a comprehensive evaluation of the model's performance. Since we only have one class, it is identical to the AP in this example.\n",
        "\n",
        "* F1 Score: The F1 Score is the harmonic mean of precision and recall, providing a balanced assessment of a model's performance while considering both false positives and false negatives.\n",
        "\n",
        "See: https://docs.ultralytics.com/guides/yolo-performance-metrics/#object-detection-metrics\n",
        "\n",
        "</details>\n",
        "\n",
        "## Testing the model\n",
        "Now you can import your newly trained model in order to test it. You can use it just like any other YOLO model. The model is saved in the `runs/train/trainX/weights/` directory. Make sure to replace `trainX` with the last training run you did or use the function below to get the latest model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNl-Dqcz80Kw"
      },
      "outputs": [],
      "source": [
        "def getLastModelPath():\n",
        "    # Get the path of the trained model\n",
        "    path = f\"{os.getcwd()}/runs/detect/\"\n",
        "    folders = os.listdir(path)\n",
        "    train_folders = [folder for folder in folders if \"train\" in folder]\n",
        "    last_train_folder = sorted(train_folders)[-1]\n",
        "    model_path = f\"{path}{last_train_folder}/weights/\"\n",
        "    print(f\"Last train folder: {model_path}\")\n",
        "    return model_path\n",
        "\n",
        "model_path = getLastModelPath()\n",
        "\n",
        "# Use newly trained model for inference\n",
        "model = YOLO(f\"{model_path}/best.pt\")\n",
        "\n",
        "# Test the model on an image\n",
        "image_url = \"https://www.graubuenden.ch/sites/graubuenden/files/styles/hero_xlarge_2x/public/2022-11/skifahren-davos-klosters-skigebiet-parsenn.jpg\"\n",
        "results = model(image_url)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt-4A5fQ80Kw"
      },
      "source": [
        "As you can see, the new model should perform well on our test image and should be able to detect our skiers.ü•≥\n",
        "\n",
        "**Exercise 1:** Do you know why the model performs well with only these few images? Would it work as well with the same amount of images if we trained it on something completely different (e.g. a paraglider or cyclist)? Why or why not? *Hint: Look at the training data of the original YOLO model.*\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "    \n",
        "In the COCO dataset (https://cocodataset.org/#explore) that was used to train the YOLO model, there are already many images of people and skis, so it is not very difficult for the model to recognize skiers, even with relatively few training images. If you want to detect something completely different, that is not similar to objects in the COCO dataset, you might need a lot more training data, as the model has never seen this object before and has no idea what it looks like.\n",
        "</details>\n",
        "\n",
        "**Exercise 2:** Did you figure out why the unlabeled images of regular people were in the dataset? What would happen if we didn't include them in the dataset?\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "\n",
        "The images of people were included in the dataset to prevent the model from detecting all people as skiers. Since the original model is able to detect people and skis, without these images, the model would make the association that all people are skiers and vice versa.\n",
        "\n",
        "</details>\n",
        "\n",
        "**Exercise 3:** Can you test it on some challenging images (e.g. from this video https://www.youtube.com/watch?v=B5xckyNsWKw)? How does it perform? What are the limitations of the model and how could you improve it? If you want you can also use the model to track a skier in a short video.\n",
        "\n",
        "<details>\n",
        "    <summary>Click here to hide/unhide the answer!</summary>\n",
        "The model performs well on images/videos that are similar to the training data, however if you test it on images/videos that are different from the training data, the performance is not very good. For example with a video from a ski race from the perspective of a drone/helicopter or with different lighting/weather, the skiers are not reliably recognized. In order to improve the model, it's always good to have a diverse dataset with images from different perspectives, lighting conditions, etc. This way the model can generalize better and perform well on unseen data. Large datasets (e.g. the COCO dataset used to train YOLO) can contain tens of thousands of images, which takes a lot of effort to create and label. So generally speaking, you have to test the model and see how it performs on your specific use case and then decide if you need more data to improve the model or not.\n",
        "\n",
        "Sometimes you can also get creative, as can be seen with synthetic data. For example you could take the COCO dataset and combine the bounding boxes of people and skis to get a dataset of several thousand images of people skiing.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYeUJOxB80Kw"
      },
      "outputs": [],
      "source": [
        "# Insert your code here ‚úèÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j4SBMqD80Kx"
      },
      "source": [
        "## Outlook: Deploying the model on hardware\n",
        "After you have trained your model, you can now deploy it in the field. While training models is computationally very expensive, inference can be done with a lot less compute power. For example, YOLO can even run low-power devices like smartphones, if they have hardware acceleration. The YOLOv8s model runs at 80 FPS on a Raspberry Pi 5 with AI kit using only 6W of power. Without AI accelerator, the model can also run on the CPU, however performance is limited at around [1 FPS](https://docs.ultralytics.com/guides/raspberry-pi/#__tabbed_2_2), which can still be useful, depending on the application.\n",
        "\n",
        "\n",
        "Depending on the requirements, you can use edge or cloud computing:\n",
        "\n",
        "Edge computing means running models directly on a local device. This approach offers benefits like improved privacy (since data doesn‚Äôt leave the device), reduced latency (quicker response times), and lower ongoing costs. However, edge devices typically have higher power draw and larger upfront cost.\n",
        "\n",
        "On the other hand, cloud computing refers to using remote servers for processing. Cloud services provide scalable power, ideal for more intensive or burst workloads, but they come with recurring costs, higher latency, a need for constant connectivity and can be a privacy concern. This can be useful, if you want very low cost- and power hardware and have the connectivity, to send data to the cloud for processing or want to use very large models (e.g. ChatGPT).\n",
        "\n",
        "![onnx](https://www.aurigait.com/wp-content/uploads/2023/01/unnamed-1.png)\n",
        "\n",
        "As you saw with the exercises, the hardware you use can have a significant impact on model speed. There are many different hardware accelerators for AI (GPU, TPU, NPU, FPGA, ‚Ä¶) and unfortunately, a lot of software only works with specific hardware architecture. For example the Ultralytics YOLO library only works with NVIDIA and not AMD graphics cards out of the box. Because of this, there are open standards like ONNX (Open Neural Network Exchange), that can help with converting and makes running models on different hardware a lot easier. You can see the command to convert a model to ONNX below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZhFOviW80Kx"
      },
      "outputs": [],
      "source": [
        "# Export model as ONNX\n",
        "# https://docs.ultralytics.com/modes/export/#how-do-i-export-a-yolov8-model-to-onnx-format\n",
        "onnx_model_path = model.export(format=\"onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0XLrU080Kx"
      },
      "source": [
        "You can also use the https://netron.app/ tool to visualize the model architecture.\n",
        "\n",
        "Using hardware accelerators can also sometimes lead to special challenges, for example when you want to do object detection on the GPU and then process the results on your CPU. Additionally, when running models on edge devices, optimization is key to improving performance and efficiency when deploying your model. Using techniques like pruning, half precision or batch processing can significantly speed up operations and are also hardware dependent.\n",
        "\n",
        "**With this, you have completed the tutorial on object detection with YOLO. I hope you enjoyed the tutorial and learned something new. Feel free to give me feedback on how to improve this course! Good luck with your own projects!** üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}